{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bfdccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fa38ee",
   "metadata": {},
   "source": [
    "Device dignostics code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d284dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018cb9c6",
   "metadata": {},
   "source": [
    "Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9c7ea9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"E:\\Pytorch Datasets\\Classification\\wine+quality\\winequality-white.csv\", delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "200b77bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      8.8        6  \n",
       "1      9.5        6  \n",
       "2     10.1        6  \n",
       "3      9.9        6  \n",
       "4      9.9        6  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e3cae3",
   "metadata": {},
   "source": [
    "Filtering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01dafd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df.iloc[:, :-1])\n",
    "y = np.array(df.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "369c4851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, numpy.ndarray)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X), type(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5726d8e8",
   "metadata": {},
   "source": [
    "Converting numpy to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7efb6167",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X).type(torch.float)\n",
    "y = torch.from_numpy(y).type(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7e032e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Tensor)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X), type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f85637ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4898, 11]), torch.Size([4898]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc92b72",
   "metadata": {},
   "source": [
    "Spliting training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6700faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5261c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3918, 11]),\n",
       " torch.Size([980, 11]),\n",
       " torch.Size([3918]),\n",
       " torch.Size([980]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a648eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X_train.to(device), X_test.to(device), y_train.to(device), y_test.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02449f6",
   "metadata": {},
   "source": [
    "Build a Linear model by inheriting nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18b92be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(in_features=11, out_features=128)\n",
    "        self.layer2 = nn.Linear(in_features=128, out_features=256)\n",
    "        self.layer3 = nn.Linear(in_features=256, out_features=512)\n",
    "        self.layer4 = nn.Linear(in_features=512, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        return x\n",
    "    \n",
    "model = Model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca28fd0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer1.weight',\n",
       "              tensor([[-0.2319, -0.1943, -0.0149,  ..., -0.1029,  0.0196, -0.2279],\n",
       "                      [-0.0317,  0.1683, -0.1722,  ...,  0.0015, -0.1720, -0.2500],\n",
       "                      [-0.1589,  0.2182, -0.0194,  ..., -0.1231,  0.1058, -0.1237],\n",
       "                      ...,\n",
       "                      [-0.0744,  0.1220,  0.1641,  ..., -0.0699, -0.1357,  0.0897],\n",
       "                      [ 0.1077, -0.0468,  0.1491,  ..., -0.0498, -0.0599,  0.0766],\n",
       "                      [ 0.2349, -0.1220, -0.1470,  ..., -0.2167,  0.1084,  0.2602]])),\n",
       "             ('layer1.bias',\n",
       "              tensor([-3.0323e-02,  2.3695e-01,  2.9633e-01, -5.1952e-02,  3.6070e-02,\n",
       "                      -2.2347e-01,  1.6354e-01,  2.4711e-01,  2.7964e-01, -2.8024e-01,\n",
       "                      -2.0365e-01, -2.1766e-01, -2.1862e-01, -1.8477e-01, -1.6141e-04,\n",
       "                       1.7406e-01, -6.8942e-02, -1.7870e-01, -2.1153e-01,  1.6013e-01,\n",
       "                       2.5461e-01, -5.0715e-02,  1.1666e-02, -8.9629e-02, -8.1342e-02,\n",
       "                       2.5299e-01, -8.6935e-02,  2.5739e-01, -1.5094e-01,  5.9223e-02,\n",
       "                       2.5591e-02,  2.3940e-01, -2.3195e-01, -7.9569e-02, -8.3176e-02,\n",
       "                      -1.1439e-01, -2.0882e-02,  2.2399e-01,  8.0321e-02,  3.5917e-02,\n",
       "                      -1.0381e-01, -2.1713e-01,  2.4002e-02,  2.7942e-01, -1.1602e-01,\n",
       "                       2.3559e-01,  2.5029e-01,  2.5468e-01,  1.9957e-02, -9.0481e-02,\n",
       "                      -3.0642e-03, -1.9835e-01, -1.7698e-01, -1.0419e-01,  1.9739e-01,\n",
       "                      -2.7907e-01,  2.7152e-02,  1.3033e-01, -2.8960e-01, -2.0786e-02,\n",
       "                      -1.8251e-01,  2.7595e-01,  5.4806e-02,  2.5345e-01,  2.6733e-01,\n",
       "                       2.8683e-01,  1.5571e-01, -1.8303e-01, -8.2390e-02, -2.4889e-01,\n",
       "                       2.0091e-01, -1.3020e-01, -6.4342e-02,  1.6937e-01, -2.7712e-03,\n",
       "                       2.0398e-01,  5.6527e-02,  1.2630e-01, -6.1904e-02,  1.7856e-01,\n",
       "                      -1.8120e-01,  8.2750e-02, -1.1466e-01, -7.9087e-02, -1.2723e-01,\n",
       "                       3.4550e-02,  1.5112e-01,  2.0719e-01, -7.8414e-02,  2.6829e-01,\n",
       "                      -1.4587e-01, -1.4909e-01,  1.6484e-01,  1.9532e-01, -2.9044e-01,\n",
       "                       6.0724e-02, -1.7977e-01,  1.7455e-01,  1.4786e-01,  1.6741e-01,\n",
       "                       1.1237e-01,  1.2145e-01,  9.6819e-04, -2.7923e-01, -1.4833e-01,\n",
       "                       1.4768e-01,  7.8575e-02, -7.1686e-02, -3.4005e-02,  2.0452e-01,\n",
       "                      -2.9374e-01,  7.3713e-02, -2.0780e-01,  7.8749e-02, -1.6478e-01,\n",
       "                      -2.8886e-01,  1.6734e-01, -7.4937e-02, -3.0026e-01,  2.0814e-01,\n",
       "                       2.6345e-01, -2.7151e-01,  1.5959e-01, -2.6214e-01,  9.7127e-02,\n",
       "                      -2.0297e-01, -1.7160e-01, -2.0212e-01])),\n",
       "             ('layer2.weight',\n",
       "              tensor([[-0.0629, -0.0695,  0.0827,  ...,  0.0354, -0.0031,  0.0449],\n",
       "                      [-0.0338, -0.0674, -0.0241,  ...,  0.0211, -0.0306,  0.0839],\n",
       "                      [-0.0351,  0.0803,  0.0372,  ..., -0.0243,  0.0339, -0.0875],\n",
       "                      ...,\n",
       "                      [ 0.0763, -0.0588, -0.0763,  ...,  0.0783,  0.0601, -0.0362],\n",
       "                      [ 0.0298,  0.0659, -0.0291,  ...,  0.0157,  0.0516,  0.0677],\n",
       "                      [ 0.0643,  0.0706, -0.0769,  ...,  0.0432,  0.0690, -0.0613]])),\n",
       "             ('layer2.bias',\n",
       "              tensor([ 0.0147, -0.0568,  0.0765, -0.0187,  0.0614, -0.0350, -0.0699,  0.0336,\n",
       "                      -0.0834,  0.0410, -0.0136,  0.0240, -0.0627,  0.0052, -0.0341, -0.0774,\n",
       "                      -0.0736, -0.0064,  0.0545, -0.0610,  0.0554, -0.0653,  0.0712,  0.0109,\n",
       "                       0.0400, -0.0400, -0.0343,  0.0351,  0.0328,  0.0247,  0.0696,  0.0137,\n",
       "                      -0.0264, -0.0095,  0.0735,  0.0419,  0.0206,  0.0078, -0.0813, -0.0141,\n",
       "                       0.0431, -0.0599, -0.0003,  0.0232, -0.0859, -0.0518, -0.0640,  0.0594,\n",
       "                      -0.0591, -0.0762, -0.0190,  0.0308, -0.0460,  0.0362, -0.0326, -0.0439,\n",
       "                      -0.0228,  0.0249,  0.0464, -0.0581, -0.0382, -0.0461, -0.0520,  0.0614,\n",
       "                      -0.0210,  0.0261,  0.0193, -0.0383, -0.0676, -0.0217,  0.0313, -0.0746,\n",
       "                      -0.0400, -0.0627,  0.0190, -0.0391, -0.0228,  0.0210, -0.0315, -0.0809,\n",
       "                      -0.0820, -0.0615, -0.0740,  0.0794,  0.0482,  0.0552,  0.0868,  0.0338,\n",
       "                       0.0141,  0.0619, -0.0335,  0.0865, -0.0733, -0.0256, -0.0182,  0.0826,\n",
       "                      -0.0501,  0.0352, -0.0435, -0.0132,  0.0406, -0.0863, -0.0342, -0.0154,\n",
       "                       0.0648,  0.0006, -0.0207, -0.0204, -0.0103,  0.0816, -0.0570,  0.0392,\n",
       "                       0.0211,  0.0084, -0.0803, -0.0505, -0.0075,  0.0703,  0.0063,  0.0004,\n",
       "                      -0.0058,  0.0364,  0.0003,  0.0636, -0.0235,  0.0036, -0.0685,  0.0592,\n",
       "                       0.0437,  0.0348, -0.0727,  0.0637, -0.0531,  0.0407,  0.0627, -0.0184,\n",
       "                      -0.0163, -0.0498,  0.0150,  0.0130, -0.0128,  0.0801, -0.0171, -0.0758,\n",
       "                       0.0104, -0.0299,  0.0242, -0.0699, -0.0096,  0.0109, -0.0614,  0.0828,\n",
       "                       0.0182,  0.0047, -0.0701, -0.0871,  0.0193, -0.0310, -0.0873, -0.0269,\n",
       "                       0.0758,  0.0535,  0.0049,  0.0170, -0.0592,  0.0290, -0.0719,  0.0166,\n",
       "                       0.0392,  0.0773, -0.0808, -0.0269,  0.0691,  0.0638,  0.0700, -0.0129,\n",
       "                      -0.0734, -0.0699, -0.0162,  0.0227,  0.0253, -0.0574, -0.0330, -0.0513,\n",
       "                       0.0841, -0.0829, -0.0776,  0.0792, -0.0074,  0.0103,  0.0683,  0.0673,\n",
       "                      -0.0490,  0.0538, -0.0653,  0.0314, -0.0760,  0.0748, -0.0390, -0.0385,\n",
       "                      -0.0738,  0.0168,  0.0442, -0.0765, -0.0185,  0.0868, -0.0155,  0.0849,\n",
       "                       0.0232,  0.0718, -0.0162, -0.0880, -0.0827,  0.0475,  0.0215, -0.0405,\n",
       "                       0.0245,  0.0316,  0.0835,  0.0013, -0.0185,  0.0298,  0.0012,  0.0600,\n",
       "                       0.0391,  0.0646,  0.0247,  0.0856,  0.0050,  0.0184, -0.0863,  0.0501,\n",
       "                       0.0524, -0.0087, -0.0670, -0.0238, -0.0085,  0.0127,  0.0114,  0.0094,\n",
       "                       0.0880,  0.0706, -0.0261, -0.0525,  0.0625, -0.0126, -0.0521, -0.0781,\n",
       "                      -0.0505,  0.0192,  0.0031,  0.0167,  0.0727, -0.0344,  0.0440, -0.0712])),\n",
       "             ('layer3.weight',\n",
       "              tensor([[ 0.0339, -0.0623, -0.0237,  ..., -0.0030,  0.0291, -0.0200],\n",
       "                      [-0.0416,  0.0070,  0.0384,  ..., -0.0369,  0.0503,  0.0370],\n",
       "                      [-0.0275, -0.0333,  0.0483,  ..., -0.0065, -0.0353,  0.0435],\n",
       "                      ...,\n",
       "                      [ 0.0176,  0.0463, -0.0110,  ..., -0.0533, -0.0395, -0.0329],\n",
       "                      [-0.0549, -0.0008, -0.0166,  ...,  0.0196,  0.0451, -0.0149],\n",
       "                      [-0.0527, -0.0546, -0.0287,  ...,  0.0607,  0.0507,  0.0422]])),\n",
       "             ('layer3.bias',\n",
       "              tensor([ 0.0575, -0.0126, -0.0035, -0.0217,  0.0329,  0.0382,  0.0077,  0.0230,\n",
       "                       0.0170,  0.0571,  0.0223, -0.0330, -0.0049, -0.0516, -0.0180, -0.0201,\n",
       "                       0.0559, -0.0548, -0.0022,  0.0511,  0.0134, -0.0539,  0.0093,  0.0188,\n",
       "                       0.0079,  0.0176, -0.0354, -0.0280, -0.0076, -0.0528,  0.0088, -0.0355,\n",
       "                      -0.0362,  0.0568, -0.0049, -0.0366,  0.0567, -0.0057, -0.0226,  0.0521,\n",
       "                       0.0535, -0.0374,  0.0278, -0.0551, -0.0357, -0.0007,  0.0542, -0.0446,\n",
       "                       0.0616, -0.0294, -0.0467,  0.0607, -0.0322, -0.0195, -0.0063,  0.0167,\n",
       "                      -0.0325, -0.0587,  0.0169, -0.0268,  0.0597,  0.0400, -0.0345,  0.0447,\n",
       "                       0.0442, -0.0439,  0.0244,  0.0577,  0.0378, -0.0191, -0.0420,  0.0337,\n",
       "                       0.0465,  0.0524, -0.0119, -0.0342,  0.0062,  0.0446,  0.0273, -0.0055,\n",
       "                      -0.0471,  0.0147,  0.0142,  0.0395, -0.0417,  0.0230,  0.0198,  0.0222,\n",
       "                      -0.0444, -0.0089, -0.0509, -0.0468, -0.0468, -0.0337,  0.0066,  0.0483,\n",
       "                       0.0306, -0.0153, -0.0497,  0.0225, -0.0398, -0.0143, -0.0508, -0.0045,\n",
       "                       0.0349, -0.0058, -0.0137, -0.0451,  0.0376, -0.0011, -0.0585,  0.0314,\n",
       "                       0.0256, -0.0439,  0.0502,  0.0476, -0.0460, -0.0495, -0.0155, -0.0143,\n",
       "                       0.0395,  0.0260, -0.0224, -0.0377,  0.0067, -0.0115,  0.0617, -0.0152,\n",
       "                      -0.0508, -0.0115, -0.0062,  0.0498, -0.0124, -0.0568,  0.0377,  0.0424,\n",
       "                      -0.0262, -0.0465, -0.0118,  0.0009,  0.0519,  0.0180, -0.0225, -0.0510,\n",
       "                       0.0124, -0.0267,  0.0561,  0.0371, -0.0397,  0.0243,  0.0041,  0.0034,\n",
       "                       0.0320, -0.0080,  0.0541, -0.0084, -0.0438,  0.0276, -0.0284,  0.0418,\n",
       "                      -0.0318, -0.0348,  0.0417, -0.0039, -0.0417, -0.0274,  0.0302, -0.0291,\n",
       "                       0.0546, -0.0377, -0.0194, -0.0582, -0.0590,  0.0596, -0.0490,  0.0406,\n",
       "                      -0.0473,  0.0315, -0.0226,  0.0375,  0.0477,  0.0474, -0.0382,  0.0164,\n",
       "                       0.0603, -0.0600,  0.0545,  0.0254,  0.0307,  0.0318,  0.0103,  0.0393,\n",
       "                      -0.0570,  0.0520, -0.0234,  0.0601, -0.0586, -0.0457,  0.0603,  0.0589,\n",
       "                       0.0505, -0.0181,  0.0586, -0.0600,  0.0581,  0.0120,  0.0259,  0.0381,\n",
       "                       0.0130, -0.0259,  0.0518,  0.0354, -0.0614,  0.0270, -0.0188, -0.0418,\n",
       "                      -0.0422, -0.0070,  0.0175, -0.0173,  0.0297,  0.0063,  0.0422, -0.0556,\n",
       "                      -0.0465, -0.0410,  0.0319,  0.0041,  0.0348,  0.0452, -0.0028,  0.0548,\n",
       "                      -0.0120,  0.0450, -0.0412,  0.0253, -0.0464,  0.0340,  0.0454, -0.0077,\n",
       "                      -0.0490, -0.0538, -0.0500,  0.0462,  0.0326, -0.0477, -0.0422,  0.0078,\n",
       "                       0.0383,  0.0169, -0.0230,  0.0511, -0.0566, -0.0207, -0.0382,  0.0093,\n",
       "                       0.0345, -0.0530,  0.0150,  0.0589, -0.0020, -0.0172, -0.0356,  0.0054,\n",
       "                       0.0409, -0.0404, -0.0185,  0.0378, -0.0505,  0.0008,  0.0369, -0.0094,\n",
       "                       0.0493, -0.0505, -0.0416,  0.0348,  0.0534, -0.0411,  0.0249,  0.0013,\n",
       "                       0.0241,  0.0428,  0.0397, -0.0354, -0.0073,  0.0429,  0.0254,  0.0030,\n",
       "                       0.0568, -0.0018, -0.0417,  0.0121, -0.0400, -0.0196, -0.0573, -0.0120,\n",
       "                       0.0441, -0.0557,  0.0021,  0.0437, -0.0093, -0.0135,  0.0602, -0.0473,\n",
       "                       0.0207,  0.0368,  0.0486,  0.0366, -0.0348, -0.0519, -0.0384,  0.0013,\n",
       "                       0.0094, -0.0615,  0.0445, -0.0293,  0.0605, -0.0079, -0.0004, -0.0356,\n",
       "                       0.0548, -0.0244, -0.0426, -0.0121,  0.0384,  0.0543,  0.0552, -0.0479,\n",
       "                       0.0043,  0.0534,  0.0255, -0.0266, -0.0482,  0.0086,  0.0100, -0.0166,\n",
       "                       0.0374, -0.0454, -0.0327, -0.0360,  0.0546, -0.0541,  0.0518,  0.0251,\n",
       "                       0.0546,  0.0207,  0.0259, -0.0001,  0.0471,  0.0100,  0.0586, -0.0053,\n",
       "                      -0.0535, -0.0355,  0.0074,  0.0092,  0.0251,  0.0355, -0.0215, -0.0552,\n",
       "                      -0.0070, -0.0465, -0.0414, -0.0147,  0.0440,  0.0171, -0.0452,  0.0418,\n",
       "                       0.0075, -0.0048,  0.0066, -0.0051,  0.0096, -0.0449,  0.0276,  0.0342,\n",
       "                      -0.0135,  0.0548, -0.0297,  0.0565,  0.0263,  0.0489,  0.0594, -0.0524,\n",
       "                       0.0475,  0.0228, -0.0607, -0.0563,  0.0171, -0.0055, -0.0586, -0.0088,\n",
       "                      -0.0395,  0.0124, -0.0125,  0.0382, -0.0083, -0.0170,  0.0070, -0.0296,\n",
       "                       0.0006, -0.0463, -0.0150,  0.0587,  0.0075,  0.0192, -0.0427,  0.0481,\n",
       "                       0.0101,  0.0063,  0.0412,  0.0282,  0.0187, -0.0278,  0.0574,  0.0597,\n",
       "                      -0.0472,  0.0222,  0.0417, -0.0395, -0.0296, -0.0188,  0.0567,  0.0298,\n",
       "                      -0.0536, -0.0618,  0.0109,  0.0155, -0.0137,  0.0612, -0.0061,  0.0185,\n",
       "                      -0.0496, -0.0033,  0.0519, -0.0512, -0.0450, -0.0431,  0.0141, -0.0265,\n",
       "                      -0.0166,  0.0184, -0.0527,  0.0229,  0.0517,  0.0524,  0.0307,  0.0369,\n",
       "                       0.0244,  0.0296,  0.0506,  0.0110,  0.0597, -0.0047, -0.0020,  0.0183,\n",
       "                      -0.0423, -0.0281, -0.0298,  0.0054, -0.0382, -0.0075, -0.0239, -0.0427,\n",
       "                       0.0021,  0.0056, -0.0436,  0.0449, -0.0041,  0.0537,  0.0329, -0.0244,\n",
       "                      -0.0135, -0.0197, -0.0260, -0.0393, -0.0603,  0.0007, -0.0341,  0.0201,\n",
       "                      -0.0455, -0.0069,  0.0171,  0.0044, -0.0615,  0.0168, -0.0099, -0.0391,\n",
       "                       0.0067,  0.0617, -0.0244, -0.0083, -0.0474,  0.0124,  0.0052,  0.0517,\n",
       "                       0.0446, -0.0354, -0.0142, -0.0448,  0.0335,  0.0262,  0.0457, -0.0588,\n",
       "                      -0.0416,  0.0616, -0.0526, -0.0234, -0.0597, -0.0478, -0.0507,  0.0255])),\n",
       "             ('layer4.weight',\n",
       "              tensor([[-0.0038,  0.0201, -0.0179,  ..., -0.0241, -0.0354,  0.0175],\n",
       "                      [ 0.0360,  0.0224,  0.0135,  ...,  0.0258, -0.0043, -0.0428],\n",
       "                      [ 0.0093,  0.0317,  0.0082,  ..., -0.0185, -0.0111,  0.0412],\n",
       "                      ...,\n",
       "                      [-0.0317, -0.0287, -0.0147,  ...,  0.0395, -0.0106,  0.0107],\n",
       "                      [ 0.0397, -0.0104, -0.0077,  ..., -0.0070,  0.0291, -0.0147],\n",
       "                      [ 0.0069, -0.0156, -0.0304,  ..., -0.0345, -0.0351,  0.0355]])),\n",
       "             ('layer4.bias',\n",
       "              tensor([ 0.0401, -0.0057,  0.0330, -0.0069, -0.0304,  0.0167, -0.0418, -0.0223,\n",
       "                      -0.0367, -0.0086]))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdede71",
   "metadata": {},
   "source": [
    "Testing everything is fine using dummy values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd344a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    y_logits = model(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b4ecc69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([980, 10])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6dcceef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = torch.argmax(y_logits, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5ac93b",
   "metadata": {},
   "source": [
    "Choosing Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f8aeffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    params=model.parameters()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cef7e9b",
   "metadata": {},
   "source": [
    "Writing Training and Test loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc34c685",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.long()\n",
    "y_test = y_test.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d5c4c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLoss, testLoss = [], []\n",
    "trainAccuracy, testAccuracy = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7922a318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Train Loss: 4.95140 | Train Accuracy: 0.4556 | Test Loss: 6.44115 | Test Accuracy: 0.3102\n",
      "Epoch: 20 | Train Loss: 5.48863 | Train Accuracy: 0.4558 | Test Loss: 5.23269 | Test Accuracy: 0.4214\n",
      "Epoch: 30 | Train Loss: 2.12466 | Train Accuracy: 0.4033 | Test Loss: 2.31976 | Test Accuracy: 0.4082\n",
      "Epoch: 40 | Train Loss: 1.52965 | Train Accuracy: 0.4553 | Test Loss: 1.51719 | Test Accuracy: 0.4612\n",
      "Epoch: 50 | Train Loss: 1.38019 | Train Accuracy: 0.3762 | Test Loss: 1.27640 | Test Accuracy: 0.4500\n",
      "Epoch: 60 | Train Loss: 1.30052 | Train Accuracy: 0.4566 | Test Loss: 1.28417 | Test Accuracy: 0.4622\n",
      "Epoch: 70 | Train Loss: 1.24807 | Train Accuracy: 0.4607 | Test Loss: 1.24592 | Test Accuracy: 0.4571\n",
      "Epoch: 80 | Train Loss: 1.23284 | Train Accuracy: 0.4620 | Test Loss: 1.22979 | Test Accuracy: 0.4633\n",
      "Epoch: 90 | Train Loss: 1.22761 | Train Accuracy: 0.4602 | Test Loss: 1.22621 | Test Accuracy: 0.4551\n",
      "Epoch: 100 | Train Loss: 1.22455 | Train Accuracy: 0.4617 | Test Loss: 1.22526 | Test Accuracy: 0.4622\n",
      "Epoch: 110 | Train Loss: 1.22175 | Train Accuracy: 0.4594 | Test Loss: 1.22069 | Test Accuracy: 0.4571\n",
      "Epoch: 120 | Train Loss: 1.21948 | Train Accuracy: 0.4615 | Test Loss: 1.21799 | Test Accuracy: 0.4602\n",
      "Epoch: 130 | Train Loss: 1.21734 | Train Accuracy: 0.4609 | Test Loss: 1.21570 | Test Accuracy: 0.4582\n",
      "Epoch: 140 | Train Loss: 1.21526 | Train Accuracy: 0.4615 | Test Loss: 1.21339 | Test Accuracy: 0.4541\n",
      "Epoch: 150 | Train Loss: 1.21322 | Train Accuracy: 0.4604 | Test Loss: 1.21121 | Test Accuracy: 0.4551\n",
      "Epoch: 160 | Train Loss: 1.21121 | Train Accuracy: 0.4607 | Test Loss: 1.20914 | Test Accuracy: 0.4520\n",
      "Epoch: 170 | Train Loss: 1.20922 | Train Accuracy: 0.4607 | Test Loss: 1.20686 | Test Accuracy: 0.4510\n",
      "Epoch: 180 | Train Loss: 1.20724 | Train Accuracy: 0.4594 | Test Loss: 1.20467 | Test Accuracy: 0.4500\n",
      "Epoch: 190 | Train Loss: 1.20527 | Train Accuracy: 0.4584 | Test Loss: 1.20265 | Test Accuracy: 0.4510\n",
      "Epoch: 200 | Train Loss: 1.20331 | Train Accuracy: 0.4561 | Test Loss: 1.20068 | Test Accuracy: 0.4531\n",
      "Epoch: 210 | Train Loss: 1.20137 | Train Accuracy: 0.4592 | Test Loss: 1.19875 | Test Accuracy: 0.4612\n",
      "Epoch: 220 | Train Loss: 1.19944 | Train Accuracy: 0.4609 | Test Loss: 1.19690 | Test Accuracy: 0.4622\n",
      "Epoch: 230 | Train Loss: 1.19752 | Train Accuracy: 0.4645 | Test Loss: 1.19511 | Test Accuracy: 0.4633\n",
      "Epoch: 240 | Train Loss: 1.19562 | Train Accuracy: 0.4663 | Test Loss: 1.19338 | Test Accuracy: 0.4663\n",
      "Epoch: 250 | Train Loss: 1.19373 | Train Accuracy: 0.4661 | Test Loss: 1.19171 | Test Accuracy: 0.4694\n",
      "Epoch: 260 | Train Loss: 1.19186 | Train Accuracy: 0.4661 | Test Loss: 1.19010 | Test Accuracy: 0.4714\n",
      "Epoch: 270 | Train Loss: 1.19001 | Train Accuracy: 0.4666 | Test Loss: 1.18856 | Test Accuracy: 0.4694\n",
      "Epoch: 280 | Train Loss: 1.18818 | Train Accuracy: 0.4696 | Test Loss: 1.18707 | Test Accuracy: 0.4673\n",
      "Epoch: 290 | Train Loss: 1.18637 | Train Accuracy: 0.4722 | Test Loss: 1.18563 | Test Accuracy: 0.4724\n",
      "Epoch: 300 | Train Loss: 1.18458 | Train Accuracy: 0.4773 | Test Loss: 1.18425 | Test Accuracy: 0.4724\n",
      "Epoch: 310 | Train Loss: 1.18280 | Train Accuracy: 0.4763 | Test Loss: 1.18291 | Test Accuracy: 0.4694\n",
      "Epoch: 320 | Train Loss: 1.18105 | Train Accuracy: 0.4819 | Test Loss: 1.18162 | Test Accuracy: 0.4776\n",
      "Epoch: 330 | Train Loss: 1.17931 | Train Accuracy: 0.4819 | Test Loss: 1.18037 | Test Accuracy: 0.4796\n",
      "Epoch: 340 | Train Loss: 1.18227 | Train Accuracy: 0.4681 | Test Loss: 1.20036 | Test Accuracy: 0.4939\n",
      "Epoch: 350 | Train Loss: 1.36796 | Train Accuracy: 0.4592 | Test Loss: 1.21910 | Test Accuracy: 0.4296\n",
      "Epoch: 360 | Train Loss: 1.18393 | Train Accuracy: 0.4941 | Test Loss: 1.25643 | Test Accuracy: 0.4143\n",
      "Epoch: 370 | Train Loss: 1.19589 | Train Accuracy: 0.4604 | Test Loss: 1.19479 | Test Accuracy: 0.5051\n",
      "Epoch: 380 | Train Loss: 1.18937 | Train Accuracy: 0.4908 | Test Loss: 1.19097 | Test Accuracy: 0.4980\n",
      "Epoch: 390 | Train Loss: 1.18117 | Train Accuracy: 0.4709 | Test Loss: 1.18820 | Test Accuracy: 0.4469\n",
      "Epoch: 400 | Train Loss: 1.17948 | Train Accuracy: 0.4645 | Test Loss: 1.18253 | Test Accuracy: 0.4847\n",
      "Epoch: 410 | Train Loss: 1.17667 | Train Accuracy: 0.4788 | Test Loss: 1.17989 | Test Accuracy: 0.4633\n",
      "Epoch: 420 | Train Loss: 1.17446 | Train Accuracy: 0.4819 | Test Loss: 1.17913 | Test Accuracy: 0.4786\n",
      "Epoch: 430 | Train Loss: 1.17227 | Train Accuracy: 0.4824 | Test Loss: 1.17715 | Test Accuracy: 0.4806\n",
      "Epoch: 440 | Train Loss: 1.17033 | Train Accuracy: 0.4852 | Test Loss: 1.17540 | Test Accuracy: 0.4806\n",
      "Epoch: 450 | Train Loss: 1.16849 | Train Accuracy: 0.4875 | Test Loss: 1.17412 | Test Accuracy: 0.4837\n",
      "Epoch: 460 | Train Loss: 1.16677 | Train Accuracy: 0.4885 | Test Loss: 1.17304 | Test Accuracy: 0.4806\n",
      "Epoch: 470 | Train Loss: 1.16514 | Train Accuracy: 0.4903 | Test Loss: 1.17187 | Test Accuracy: 0.4796\n",
      "Epoch: 480 | Train Loss: 1.16356 | Train Accuracy: 0.4898 | Test Loss: 1.17079 | Test Accuracy: 0.4776\n",
      "Epoch: 490 | Train Loss: 1.16203 | Train Accuracy: 0.4900 | Test Loss: 1.16978 | Test Accuracy: 0.4745\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m#training\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m----> 7\u001b[0m     y_logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(y_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     10\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m loss_fn(y_logits, y_train)\n",
      "File \u001b[1;32mc:\\Users\\tharu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tharu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[13], line 12\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[0;32m     11\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[1;32m---> 12\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(x)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\tharu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tharu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\tharu\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 751\n",
    "\n",
    "for i in range(1, epochs):\n",
    "    #training\n",
    "    model.train()\n",
    "\n",
    "    y_logits = model(X_train)\n",
    "    y_pred = torch.argmax(y_logits, dim=1)\n",
    "\n",
    "    train_loss = loss_fn(y_logits, y_train)\n",
    "    train_acc = accuracy_score(y_train, y_pred)\n",
    "\n",
    "    trainLoss.append(train_loss)\n",
    "    trainAccuracy.append(train_acc)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    train_loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    #testing \n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        test_logits = model(X_test)\n",
    "        test_pred = torch.argmax(test_logits, dim=1)\n",
    "\n",
    "        test_loss = loss_fn( test_logits, y_test)\n",
    "        test_acc = accuracy_score(y_test, test_pred)\n",
    "\n",
    "        testLoss.append(test_loss)\n",
    "        testAccuracy.append(test_acc)\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Epoch: {i} | Train Loss: {train_loss:.5f} | Train Accuracy: {train_acc:.4f} | Test Loss: {test_loss:.5f} | Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a86dcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLoss = torch.tensor(trainLoss)\n",
    "testLoss = torch.tensor(testLoss)\n",
    "trainAccuracy = torch.tensor(trainAccuracy)\n",
    "testAccuracy = torch.tensor(testAccuracy)\n",
    "\n",
    "trainLoss = trainLoss.detach().numpy()\n",
    "testLoss = testLoss.detach().numpy()\n",
    "trainAccuracy = trainAccuracy.detach().numpy()\n",
    "testAccuracy = testAccuracy.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab99f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1, epochs), trainLoss, 'r-', label=\"Training loss\")\n",
    "# plt.plot(range(1, epochs), testLoss, 'b-', label=\"Testing loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2993d1d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
